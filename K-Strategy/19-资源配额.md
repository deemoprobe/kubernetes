# Kubernetes策略之资源配额和QoS

2021-0810

## 概念

多用户共享同一集群节点时，为了提高资源的利用率，对该集群的资源进行合理分配和限制。通过 `ResourceQuota` 对象定义，对每个Namespace的资源消耗总量提供限制。也可以限制Namespace中API对象的总数目上限，也可以限制Namespace中资源对象对计算机资源的使用量（CPU/内存/磁盘空间等）

ResourceQuota 对象命名规则：

- 不能超过253个字符
- 只能包含字母数字,以及'-' 和 '.'
- 须以字母或数字开头
- 须以字母或数字结尾

在集群容量小于各命名空间配额总和的情况下，可能存在资源竞争。资源竞争时，Kubernetes 系统会遵循先到先得的原则。不管是资源竞争还是配额的修改，都不会影响已经创建的资源使用对象。

## 启用资源配额

ResourceQuota默认开启，如果未启用，kubeadm集群可在默认的apiserver配置文件`/etc/kubernetes/manifests/kube-apiserver.yaml`文件中`command`下面的`kube-apiserver --enable-admission-plugins`参数后添加`ResourceQuota`(多个配置间逗号间隔)，更改后无需重启，kube-apiserver这个Pod自动加载；二进制集群则需要更改kube-apiserver守护进程的服务配置文件`/usr/lib/systemd/system/kube-apiserver.service`，在`--enable-admission-plugins=`字段后添加`ResourceQuota`(多个配置间逗号间隔)，编辑后重启kube-apiserver服务

```bash
# 以为二进制为例，守护进程配置如下
[root@k8s-master01 ~]# systemctl status kube-apiserver.service -l
● kube-apiserver.service - Kubernetes API Server
   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2022-03-17 17:51:37 CST; 7min ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 1001 (kube-apiserver)
    Tasks: 15
   Memory: 411.8M
   CGroup: /system.slice/kube-apiserver.service
           └─1001 /usr/local/bin/kube-apiserver... --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota --authorization-mode=Node,RBAC...

# 资源配额示例
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resource-test
  namespace: quotatest # 作用域
  labels:
    app: resourcequota
spec:
  hard:
    pods: 50 # 限制最多部署50个pod
    requests.cpu: 0.5 # 限制最高CPU请求数
    requests.memory: 512Mi # 最高内存请求量
    limits.cpu: 5 # CPU使用上限
    limits.memory: 16Gi # 内存使用上限
    configmaps: 20 # ConfigMap最多20个
    requests.storage: 40Gi # 最高存储请求量
    persistentvolumeclaims: 20
    replicationcontrollers: 20
    secrets: 20
    services: 50
    services.loadbalancers: "2"
    services.nodeports: "10"
```

## CPU和Memory

当然也可以对GPU进行配额(如: `requests.nvidia.com/gpu: 4`)

| 资源名称        | 描述信息                                      |
| --------------- | --------------------------------------------- |
| limits.cpu      | 所有非终止状态的Pod，CPU使用总量不能超过该值  |
| limits.memory   | 所有非终止状态的Pod，内存使用总量不能超过该值 |
| requests.cpu    | 所有非终止状态的 Pod，pod请求的cpu最大值      |
| requests.memory | 所有非终止状态的 Pod，内存请求数最大值        |
| cpu             | 与 requests.cpu 相同                          |
| memory          | 与 requests.memory 相同                       |

CPU单位换算：100m CPU，100 milliCPU 和 0.1 CPU 都相同；精度不能低于1m。1000m CPU = 1 CPU.

## 存储

可以对特定的Namespace下的存储资源总量进行限制。也可以根据相关的存储类(Storage Class)来限制存储资源

| 资源名称                                                              | 描述                                                                    |
| --------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| requests.storage                                                      | 所有存储资源的需求总量最大值                                            |
| persistentvolumeclaims                                                | 在该命名空间中所允许的 PVC 总量.                                        |
| storage-class-name.storageclass.storage.k8s.io/requests.storage       | 在所有与 storage-class-name 相关的所有PVC中,存储请求的总和不能超过该值. |
| storage-class-name.storageclass.storage.k8s.io/persistentvolumeclaims | 在与 storage-class-name 相关的所有PVC中,命名空间中可以存在的PVC总数.    |

## API对象

场景：集群中存在过多的 Secret 实际上会导致服务器和控制器无法启动。用户可以选择对 Job 进行配额管理，以防止配置不当的 CronJob 在某命名空间中创建太多 Job 而导致集群拒绝服务。或者考虑到有限的资源空间限制命名空间下Pod的数量等。

| 资源名称               | 描述                                                                                               |
| ---------------------- | -------------------------------------------------------------------------------------------------- |
| configmaps             | 在该命名空间中允许存在的 ConfigMap 总数上限.                                                       |
| persistentvolumeclaims | 在该命名空间中允许存在的 PVC 的总数上限.                                                           |
| pods                   | 在该命名空间中允许存在的非终止状态的 Pod 总数上限.Pod 终止状态.status.phase in (Failed, Succeeded) |
| replicationcontrollers | 在该命名空间中允许存在的 ReplicationController 总数上限.                                           |
| resourcequotas         | 在该命名空间中允许存在的 ResourceQuota 总数上限.                                                   |
| services               | 在该命名空间中允许存在的 Service 总数上限.                                                         |
| services.loadbalancers | 在该命名空间中允许存在的 LoadBalancer 类型的 Service 总数上限.                                     |
| services.nodeports     | 在该命名空间中允许存在的 NodePort 类型的 Service 总数上限.                                         |
| secrets                | 在该命名空间中允许存在的 Secret 总数上限.                                                          |

## 配额作用域

每个配额都有一组相关的 scope(作用域),配额只会对作用域内的资源生效. 配额机制仅统计所列举的作用域的交集中的资源用量.

当一个作用域被添加到配额中后,它会对作用域相关的资源数量作限制. 如配额中指定了允许(作用域)集合之外的资源,会导致验证错误.

| 作用域         | 描述                                                 |
| -------------- | ---------------------------------------------------- |
| Terminating    | 匹配所有 spec.activeDeadlineSeconds 不小于 0 的 Pod. |
| NotTerminating | 匹配所有 spec.activeDeadlineSeconds 是 nil 的 Pod.   |
| BestEffort     | 匹配所有 QoS 是 BestEffort 的 Pod.                   |
| NotBestEffort  | 匹配所有 QoS 不是 BestEffort 的 Pod.                 |
| PriorityClass  | 匹配所有引用了所指定的优先级类的 Pods.               |

BestEffort 跟踪以下资源：

- pods

Terminating、NotTerminating、NotBestEffort 和 PriorityClass 跟踪以下资源：

- pods
- cpu
- memory
- requests.cpu
- requests.memory
- limits.cpu
- limits.memory

`scopeSelector`字段进行作用域的配置，定义时如果使用以下值之一作为`scopeName`的值，则对应的`operator`只能是`Exists`

- Terminating
- NotTerminating
- BestEffort
- NotBestEffort

`scopeSelector`支持`PriorityClass`在`operator`字段中使用以下值：

- In
- NotIn
- Exists
- DoesNotExist

如果`operator`是`In`或`NotIn`之一，则`values`字段必须至少包含一个值，`operator`为`Exists`或`DoesNotExist`则不可以设置`values`字段

```bash
# 示例
scopeSelector:
matchExpressions:
    - scopeName: PriorityClass
    operator: In
    values:
        - high
---
scopeSelector:
matchExpressions:
    - scopeName: Terminating
    operator: Exists
```

## 资源配额实例

### 基于优先级的调度

Pod 可以创建为特定的优先级。通过使用资源配额中的`scopeSelector`字段，用户可以根据 Pod 的优先级控制其系统资源消耗。可以跟踪以下资源：

- pods
- cpu
- memory
- ephemeral-storage
- limits.cpu
- limits.memory
- limits.ephemeral-storage
- requests.cpu
- requests.memory
- requests.ephemeral-storage

实例说明:

- 创建三个优先级的资源配额: "low"、"medium"、"high"
- 匹配特定的Pod

```bash
# 获取apiVersion, resourcesquota简写为quota
[root@k8s-master01 ~]# kubectl explain quota
KIND:     ResourceQuota
VERSION:  v1

DESCRIPTION:
     ResourceQuota sets aggregate quota restrictions enforced per namespace

FIELDS:
   apiVersion   <string>
   kind <string>
   metadata     <Object>
   spec <Object>
   status       <Object>

# 创建优先级资源配额
[root@k8s-master01 ~]# mkdir yamls/quota
[root@k8s-master01 ~]# cd yamls/quota/
[root@k8s-master01 quota]# vim priorityclass-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: pods-high
spec:
  hard:
    cpu: "100"
    memory: 10Gi
    pods: "10"
  scopeSelector:
    matchExpressions:
    - scopeName: PriorityClass
      operator: In
      values:
      - high
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: pods-medium
spec:
  hard:
    cpu: "50"
    memory: 5Gi
    pods: "10"
  scopeSelector:
    matchExpressions:
    - scopeName: PriorityClass
      operator: In
      values:          
      - medium
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: pods-low
spec:
  hard:
    cpu: "10"
    memory: 1Gi
    pods: "10"
  scopeSelector:
    matchExpressions:
    - scopeName: PriorityClass
      operator: In
      values:
      - low
# 创建
[root@k8s-master01 quota]# kubectl apply -f priorityclass-quota.yaml 
resourcequota/pods-high created
resourcequota/pods-medium created
resourcequota/pods-low created
# 查看
[root@k8s-master01 quota]# kubectl get quota
NAME          AGE   REQUEST                                  LIMIT
pods-high     15s   cpu: 0/100, memory: 0/10Gi, pods: 0/10   
pods-low      15s   cpu: 0/10, memory: 0/1Gi, pods: 0/10     
pods-medium   15s   cpu: 0/50, memory: 0/5Gi, pods: 0/10 
[root@k8s-master01 quota]# kubectl describe quota
Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     100
memory      0     10Gi
pods        0     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     1Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     50
memory      0     5Gi
pods        0     10
# 启用优先级调度之前需要先创建对应的PriorityClass资源
# 查看PriorityClass的apiVersion, PriorityClass简写为pc
[root@k8s-master01 quota]# kubectl explain pc
KIND:     PriorityClass
VERSION:  scheduling.k8s.io/v1

DESCRIPTION:
     PriorityClass defines mapping from a priority class name to the priority
     integer value. The value can be any valid integer.

FIELDS:
   apiVersion   <string>

   description  <string>
     description is an arbitrary string that usually provides guidelines on when
     this priority class should be used.

   globalDefault        <boolean>
     globalDefault specifies whether this PriorityClass should be considered as
     the default priority for pods that do not have any priority class. Only one
     PriorityClass can be marked as `globalDefault`. However, if more than one
     PriorityClasses exists with their `globalDefault` field set to true, the
     smallest value of such global default PriorityClasses will be used as the
     default priority.

   kind <string>
   metadata     <Object>
   preemptionPolicy     <string>
     PreemptionPolicy is the Policy for preempting pods with lower priority. One
     of Never, PreemptLowerPriority. Defaults to PreemptLowerPriority if unset.
     This field is beta-level, gated by the NonPreemptingPriority feature-gate.

   value        <integer> -required-
     The value of this priority class. This is the actual priority that pods
     receive when they have the name of this class in their pod spec.
# 创建优先级调度指标
[root@k8s-master01 quota]# vim priorityclass.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
 name: high
value: 1000000
globalDefault: false
description: "High Priority."
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
 name: medium
value: 10000000
globalDefault: false
description: "Medium Priority."
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
 name: low
value: 100000000
globalDefault: false
description: "Low Priority."
# 创建
[root@k8s-master01 quota]# kubectl apply -f priorityclass.yaml 
priorityclass.scheduling.k8s.io/high created
priorityclass.scheduling.k8s.io/medium created
priorityclass.scheduling.k8s.io/low created
# 查看三个优先级
[root@k8s-master01 quota]# kubectl get pc
NAME                      VALUE        GLOBAL-DEFAULT   AGE
high                      1000000      false            13s
low                       100000000    false            13s
medium                    10000000     false            13s
# 创建Pod, 调用高优先级的指标
[root@k8s-master01 quota]# vim high-priority-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: high-priority
spec:
  containers:
  - name: high-priority
    image: ubuntu
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello; sleep 10;done"]
    resources:
      requests:
        memory: "1Gi"
        cpu: "50m"
      limits:
        memory: "10Gi"
        cpu: "100m"
  priorityClassName: high
[root@k8s-master01 quota]# kubectl apply -f high-priority-pod.yaml 
pod/high-priority created
# 可以看到高优先级的资源配额已经被调度使用了
[root@k8s-master01 quota]# kubectl describe quota | grep high -A 7
Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         50m   100
memory      1Gi   10Gi
pods        1     10
# 可以看到default这个Namespace也出现了资源配额的记录
[root@k8s-master01 quota]# kubectl describe ns default 
Name:         default
Labels:       kubernetes.io/metadata.name=default
Annotations:  <none>
Status:       Active

Resource Quotas
  Name:     pods-high
  Resource  Used  Hard
  --------  ---   ---
  cpu       50m   100
  memory    1Gi   10Gi
  pods      1     10
  Name:     pods-low
  Resource  Used  Hard
  --------  ---   ---
  cpu       0     10
  memory    0     1Gi
  pods      0     10
  Name:     pods-medium
  Resource  Used  Hard
  --------  ---   ---
  cpu       0     50
  memory    0     5Gi
  pods      0     10

No LimitRange resource.
```

### LimitRange

为Namespace=quotatest进行默认的资源限制, 使用LimitRange

```bash
[root@k8s-master01 quota]# kubectl explain limits
KIND:     LimitRange
VERSION:  v1

DESCRIPTION:
     LimitRange sets resource usage limits for each kind of resource in a
     Namespace.
FIELDS:
   apiVersion   <string>
   kind <string>
   metadata     <Object>
   spec <Object>
# 创建limitrange
[root@k8s-master01 quota]# vim limit-range.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
  namespace: quotatest
spec:
  limits:
  - default:
      cpu: 2
      memory: 1024Mi
    defaultRequest:
      cpu: 0.5
      memory: 256Mi
    type: Container
[root@k8s-master01 quota]# kubectl apply -f limit-range.yaml 
limitrange/limit-range created
[root@k8s-master01 quota]# kubectl describe limitranges -n quotatest limit-range 
Name:       limit-range
Namespace:  quotatest
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Container   memory    -    -    256Mi            1Gi            -
Container   cpu       -    -    500m             2              -
[root@k8s-master01 quota]# kubectl describe ns quotatest 
Name:         quotatest
Labels:       kubernetes.io/metadata.name=quotatest
Annotations:  <none>
Status:       Active

No resource quota.

Resource Limits
 Type       Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---  ---  ---------------  -------------  -----------------------
 Container  cpu       -    -    500m             2              -
 Container  memory    -    -    256Mi            1Gi            -
# 尝试突破限制
[root@k8s-master01 quota]# vim limit-pod-out.yaml
apiVersion: v1
kind: Pod
metadata:
  name: limit-pod1
  namespace: quotatest
spec:
  containers:
  - name: limit-pod1
    image: nginx
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 3
# 无法创建
[root@k8s-master01 quota]# kubectl apply -f limit-pod-out.yaml 
The Pod "limit-pod1" is invalid: spec.containers[0].resources.requests: Invalid value: "3": must be less than or equal to cpu limit
```

### 限制最大最小值

```bash
# 先删除之前的配置
[root@k8s-master01 quota]# kubectl delete limitranges -n quotatest limit-range 
limitrange "limit-range" deleted
# 创建新约束
[root@k8s-master01 quota]# vim limit-max-min.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-max-min
  namespace: quotatest
spec:
  limits:
  - default: # 为所在namespace下pod添加默认limits限制
      cpu: 1
      memory: 1G
    defaultRequest: # 为所在namespace下pod添加默认request限制
      cpu: 0.5
      memory: 256Mi
    max: # 限制所在namespace资源最大使用量
      cpu: 1
      memory: 4G
    min: # 限制所在namespace资源最低使用量，防止创建过多低资源pod
      cpu: 0.1
      memory: 100Mi
    type: Container # 容器类型
[root@k8s-master01 quota]# kubectl apply -f limit-max-min.yaml 
limitrange/limit-max-min created
[root@k8s-master01 quota]# kubectl get limitranges -n quotatest 
NAME            CREATED AT
limit-max-min   2022-03-17T10:54:00Z
[root@k8s-master01 quota]# kubectl describe ns quotatest 
Name:         quotatest
Labels:       kubernetes.io/metadata.name=quotatest
Annotations:  <none>
Status:       Active

No resource quota.

Resource Limits
 Type       Resource  Min    Max  Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---    ---  ---------------  -------------  -----------------------
 Container  cpu       100m   1    500m             1              -
 Container  memory    100Mi  4G   256Mi            1G             -
```

### Pod配额

限制Namespace quotatest下的Pod创建数量

```bash
# 创建Pod配额
[root@k8s-master01 quota]# vim pod-quota.yaml
  name: pod-quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: pod-quota
  namespace: quotatest
spec:
  hard:
    pods: 30 # 限制pod最多为30个
[root@k8s-master01 quota]# kubectl apply -f pod-quota.yaml 
resourcequota/pod-quota created
[root@k8s-master01 quota]# kubectl describe ns quotatest 
Name:         quotatest
Labels:       kubernetes.io/metadata.name=quotatest
Annotations:  <none>
Status:       Active

Resource Quotas
  Name:     pod-quota
  Resource  Used  Hard
  --------  ---   ---
  pods      1     3 # 已使用1个pod

Resource Limits
 Type       Resource  Min    Max  Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---    ---  ---------------  -------------  -----------------------
 Container  cpu       100m   1    500m             1              -
 Container  memory    100Mi  4G   256Mi            1G             -
# 创建deploy, 创建3个副本
[root@k8s-master01 quota]# kubectl create deployment quotatest-nginx --image=nginx --replicas=3 -n quotatest 
deployment.apps/quotatest-nginx created
[root@k8s-master01 quota]# kubectl get po -n quotatest 
NAME                               READY   STATUS    RESTARTS       AGE
quota-86f69f8974-gffg6             1/1     Running   11 (82m ago)   7d9h
quotatest-nginx-6fcf49777c-6w9fb   1/1     Running   0              43s
quotatest-nginx-6fcf49777c-tg747   1/1     Running   0              43s
# 有一个无法创建
[root@k8s-master01 quota]# kubectl get deployments.apps -n quotatest 
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
quota             1/1     1            1           7d9h
quotatest-nginx   2/3     2            2           75s
[root@k8s-master01 quota]# kubectl get rs -n  quotatest 
NAME                         DESIRED   CURRENT   READY   AGE
quota-86f69f8974             1         1         1       7d9h
quotatest-nginx-6fcf49777c   3         2         2       2m44s
[root@k8s-master01 quota]# kubectl describe rs -n quotatest quotatest-nginx-6fcf49777c 
Name:           quotatest-nginx-6fcf49777c
Namespace:      quotatest
Selector:       app=quotatest-nginx,pod-template-hash=6fcf49777c
Labels:         app=quotatest-nginx
                pod-template-hash=6fcf49777c
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/quotatest-nginx
Replicas:       2 current / 3 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=quotatest-nginx
           pod-template-hash=6fcf49777c
  Containers:
   nginx:
    Image:        nginx
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type             Status  Reason
  ----             ------  ------
  ReplicaFailure   True    FailedCreate
Events:
  Type     Reason            Age                  From                   Message
  ----     ------            ----                 ----                   -------
  Normal   SuccessfulCreate  2m59s                replicaset-controller  Created pod: quotatest-nginx-6fcf49777c-tg747
  Normal   SuccessfulCreate  2m59s                replicaset-controller  Created pod: quotatest-nginx-6fcf49777c-6w9fb
  Warning  FailedCreate      2m59s                replicaset-controller  Error creating: pods "quotatest-nginx-6fcf49777c-rqwmv" is forbidden: exceeded quota: pod-quota, requested: pods=1, used: pods=3, limited: pods=3
 ...
  Warning  FailedCreate      80s (x9 over 2m58s)  replicaset-controller  (combined from similar events): Error creating: pods "quotatest-nginx-6fcf49777c-mr6wd" is forbidden: exceeded quota: pod-quota, requested: pods=1, used: pods=3, limited: pods=3
# pod已耗尽
[root@k8s-master01 quota]# kubectl describe ns quotatest 
Name:         quotatest
Labels:       kubernetes.io/metadata.name=quotatest
Annotations:  <none>
Status:       Active

Resource Quotas
  Name:     pod-quota
  Resource  Used  Hard
  --------  ---   ---
  pods      3     3

Resource Limits
 Type       Resource  Min    Max  Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---    ---  ---------------  -------------  -----------------------
 Container  cpu       100m   1    500m             1              -
 Container  memory    100Mi  4G   256Mi            1G             -

# 顺便查看一下默认为pod添加的资源配额
[root@k8s-master01 quota]# kubectl get po -n quotatest 
NAME                               READY   STATUS    RESTARTS       AGE
quota-86f69f8974-gffg6             1/1     Running   11 (87m ago)   7d9h
quotatest-nginx-6fcf49777c-6w9fb   1/1     Running   0              5m55s
quotatest-nginx-6fcf49777c-tg747   1/1     Running   0              5m55s
[root@k8s-master01 quota]# kubectl get po -n quotatest quotatest-nginx-6fcf49777c-6w9fb -oyaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    cni.projectcalico.org/containerID: 90b5db760b819d87ed9f88dfb95c770eefe21dad7b911bca86fa075c07e167c7
    cni.projectcalico.org/podIP: 172.18.195.46/32
    cni.projectcalico.org/podIPs: 172.18.195.46/32
    kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for container
      nginx; cpu, memory limit for container nginx'
  creationTimestamp: "2022-03-17T11:13:18Z"
  generateName: quotatest-nginx-6fcf49777c-
  labels:
    app: quotatest-nginx
    pod-template-hash: 6fcf49777c
  name: quotatest-nginx-6fcf49777c-6w9fb
  namespace: quotatest
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: quotatest-nginx-6fcf49777c
    uid: cbc3da51-3d85-4092-bd01-4bce1222a16b
  resourceVersion: "298034"
  uid: 2e22f236-4b67-40a0-ba15-d0683bb16eed
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: nginx
    resources: # 下面是自动添加的配额，和namespace限制一致
      limits:
        cpu: "1"
        memory: 1G
      requests:
        cpu: 500m
        memory: 256Mi
  ...
```

### PVC限制

```bash
# 创建PVC限制
[root@k8s-master01 quota]# vim limit-pvc.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: storagelimits
  namespace: quotatest
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: 2Gi
    min:
      storage: 1Gi
[root@k8s-master01 quota]# kubectl apply -f limit-pvc.yaml 
limitrange/storagelimits created
[root@k8s-master01 quota]# kubectl describe ns quotatest 
Name:         quotatest
Labels:       kubernetes.io/metadata.name=quotatest
Annotations:  <none>
Status:       Active

Resource Quotas
  Name:     pod-quota
  Resource  Used  Hard
  --------  ---   ---
  pods      3     3

Resource Limits
 Type                   Resource  Min    Max  Default Request  Default Limit  Max Limit/Request Ratio
 ----                   --------  ---    ---  ---------------  -------------  -----------------------
 Container              cpu       100m   1    500m             1              -
 Container              memory    100Mi  4G   256Mi            1G             -
 PersistentVolumeClaim  storage   1Gi    2Gi  -                -              -
# 尝试申请3G
[root@k8s-master01 quota]# vim pvc-3g.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-3g
  namespace: quotatest
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
[root@k8s-master01 quota]# kubectl apply -f pvc-3g.yaml 
Error from server (Forbidden): error when creating "pvc-3g.yaml": persistentvolumeclaims "pvc-3g" is forbidden: maximum storage usage per PersistentVolumeClaim is 2Gi, but request is 3Gi
# 尝试申请500Mi
[root@k8s-master01 quota]# vim pvc-500m.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-500m
  namespace: quotatest
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
[root@k8s-master01 quota]# kubectl apply -f pvc-500m.yaml 
Error from server (Forbidden): error when creating "pvc-500m.yaml": persistentvolumeclaims "pvc-500m" is forbidden: minimum storage usage per PersistentVolumeClaim is 1Gi, but request is 500Mi
```

## QoS

Guaranteed：最高服务质量，当宿主机内存不够时，会先kill掉QoS为BestEffort和Burstable的Pod，如果内存还是不够，才会kill掉QoS为Guaranteed，该级别Pod的资源占用量一般比较明确，即requests的cpu和memory和limits的cpu和memory配置的一致。

Burstable： 服务质量低于Guaranteed，当宿主机内存不够时，会先kill掉QoS为BestEffort的Pod，如果内存还是不够之后就会kill掉QoS级别为Burstable的Pod，用来保证QoS质量为Guaranteed的Pod，该级别Pod一般知道最小资源使用量，但是当机器资源充足时，还是想尽可能的使用更多的资源，即limits字段的cpu和memory大于requests的cpu和memory的配置。

BestEffort：尽力而为，当宿主机内存不够时，首先kill的就是该QoS的Pod，用以保证Burstable和Guaranteed级别的Pod正常运行。

```bash
# QoS为Guaranteed的Pod，需求明确，limit和request一致
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "700m"
      requests:
        memory: "200Mi"
        cpu: "700m"

# QoS为Burstable的Pod示例，需求不确定，但有大致估值，limit和request不一致
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-2
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-2-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"

# QoS为BestEffort的Pod配置示例，没有resources参数
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-3
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-3-ctr
    image: nginx
```
