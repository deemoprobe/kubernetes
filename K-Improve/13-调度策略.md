# Kubernetes进阶之调度策略

2021-0724

## 污点和容忍

Taint在一类服务器上打上污点，让不能容忍这个污点的Pod不能部署在打了污点的服务器上。Toleration是让Pod容忍节点上配置的污点，可以让一些需要特殊配置的Pod能够调用到具有污点和特殊配置的节点上。

节点可以设置多个污点，pod也可以通过`tolerations:`设置多个容忍策略，节点污点策略：

- NoSchedule：禁止调度到该节点，已经在该节点上的Pod不受影响
- NoExecute：禁止调度到该节点，如果不符合这个污点，会立马被驱逐（或在一段时间后）
- PreferNoSchedule：尽量避免将Pod调度到指定的节点上，如果没有更合适的节点，可以部署到该节点

```bash
# 容忍方式：完全匹配
tolerations:
- key: "taintKey"
  operator: "Equal"
  value: "taintValue"
  effect: "NoSchedule"
# 不完全匹配
tolerations:
- key: "taintKey"
  operator: "Exists"
  effect: "NoSchedule"
# 配置容忍时间
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600

# 实例
[root@k8s-master01 ~]# mkdir -p yamls/schedule
[root@k8s-master01 ~]# cd yamls/schedule/
[root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule
node/k8s-node01 tainted
[root@k8s-master01 schedule]# kubectl describe nodes k8s-node01 | grep Taint -A 2
Taints:             ssd=true:NoSchedule
Unschedulable:      false
Lease:
[root@k8s-master01 schedule]# kubectl label nodes k8s-node01 ssd=true
[root@k8s-master01 schedule]# vim pod-toleration.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    ssd: "true"
  tolerations:
  - key: "ssd"
    operator: "Exists"
    effect: "NoSchedule"
[root@k8s-master01 schedule]# kubectl get po nginx --show-labels -owide
NAME    READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES   LABELS
nginx   1/1     Running   0          38s   172.17.125.48   k8s-node01   <none>           <none>            env=test
[root@k8s-master01 schedule]# kubectl describe po nginx | grep -i node-selector -A 4
Node-Selectors:              ssd=true
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
                             ssd:NoSchedule op=Exists
Events:

# 删除污点
# 直接使用key匹配
[root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd-
node/k8s-node01 untainted
[root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule
node/k8s-node01 tainted
# key=value:Taint全量匹配
[root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule-
node/k8s-node01 untainted
[root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule
node/k8s-node01 tainted
# key:Taint匹配
[root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd:NoSchedule-
node/k8s-node01 untainted

# 修改污点
[root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule
node/k8s-node01 tainted
[root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:PreferNoSchedule --overwrite
node/k8s-node01 modified
```

Kubernetes集群内置污点：

- `node.kubernetes.io/not-ready`：节点未准备好，相当于节点状态Ready的值为False。
- `node.kubernetes.io/unreachable`：Node Controller访问不到节点，相当于节点状态Ready的值为Unknown。
- `node.kubernetes.io/-out-of-disk`：节点磁盘耗尽。
- `node.kubernetes.io/memory-pressure`：节点存在内存压力。
- `node.kubernetes.io/disk-pressure`：节点存在磁盘压力。
- `node.kubernetes.io/network-unavailable`：节点网络不可达。
- `node.kubernetes.io/unschedulable`：节点不可调度。
- `node.cloudprovider.kubernetes.io/uninitialized`：如果Kubelet启动时指定了一个外部的cloudprovider，它将给当前节点添加一个Taint将其标记为不可用。在cloud-controller-manager的一个controller初始化这个节点后，Kubelet将删除这个Taint。

```bash
# 为了防止有时部分节点健康检查太慢，300s无法完成，可以修改为容忍3000秒后再驱逐（默认是300秒）：
tolerations:
- key: "node.kubernetes.io/unreachable"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 5000
```

## 亲和力和反亲和力

![20220315223242](https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20220315223242.png)

- requiredDuringSchedulingIgnoredDuringExecution：强制调度
- preferredDuringSchedulingIgnoredDuringExecution：优先调度

### 节点亲和力

```bash
# 先删除污点
[root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:PreferNoSchedule-
node/k8s-node01 untainted
[root@k8s-master01 schedule]# kubectl taint node k8s-node01 ssd=true:NoSchedule-
node/k8s-node01 untainted
[root@k8s-master01 schedule]# kubectl describe nodes k8s-node01 | grep Taint
Taints:             <none>
# 为k8s-node01和k8s-node02打标签disktype=ssd
[root@k8s-master01 schedule]# kubectl label nodes k8s-node01 k8s-node02 disktype=ssd
node/k8s-node01 labeled
node/k8s-node02 labeled
# 确认标签
[root@k8s-master01 schedule]# kubectl get nodes --show-labels 
NAME           STATUS   ROLES    AGE   VERSION   LABELS
k8s-master01   Ready    master   9d    v1.23.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master01,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.kubernetes.io/node=
k8s-master02   Ready    master   9d    v1.23.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master02,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.kubernetes.io/node=
k8s-master03   Ready    master   9d    v1.23.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master03,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.kubernetes.io/node=
k8s-node01     Ready    node     9d    v1.23.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/node=,ssd=true
k8s-node02     Ready    node     9d    v1.23.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node02,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/node=
# 创建节点硬亲和力调度pod实例
[root@k8s-master01 schedule]# vim node-hardaffinity.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: node-hardaffinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
[root@k8s-master01 schedule]# kubectl get po node-hardaffinity -owide
NAME                READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
node-hardaffinity   1/1     Running   0          25s   172.27.14.210   k8s-node02   <none>           <none>

# 为k8s-node01单独打上ssd:first的标签
[root@k8s-master01 schedule]# kubectl label nodes k8s-node01 ssd=first
error: 'ssd' already has a value (true), and --overwrite is false
# 之前用过ssd=true，覆盖掉即可
[root@k8s-master01 schedule]# kubectl label nodes k8s-node01 ssd=first --overwrite 
node/k8s-node01 labeled
# 创建节点软亲和力调度pod实例
apiVersion: v1
kind: Pod
metadata:
  name: node-softaffinity
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: ssd
            operator: Exists
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
[root@k8s-master01 schedule]# vim node-softaffinity.yaml 
[root@k8s-master01 schedule]# kubectl get po node-softaffinity -owide
NAME                READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
node-softaffinity   1/1     Running   0          34s   172.17.125.49   k8s-node01   <none>           <none>
```

affinity配置详解：

- requiredDuringSchedulingIgnoredDuringExecution：硬亲和力配置
  - nodeSelectorTerms：节点选择器配置，可以配置多个matchExpressions（满足其一即可）
- preferredDuringSchedulingIgnoredDuringExecution：软亲和力配置
  - weight：软亲和力的权重，权重越高优先级越大，范围1-100
  - preference：软亲和力配置项，和weight同级，可以配置多个，matchExpressions和硬亲和力一致
- operator：标签匹配的方式
  - In：相当于key = value的形式
  - NotIn：相当于key != value的形式
  - Exists：节点存在label的key为指定的值即可，无需配置values字段
  - DoesNotExist：节点不存在label的key为指定的值即可，无需配置values字段
  - Gt：大于value指定的值
  - Lt：小于value指定的值

### Pod亲和力反亲和力

- requiredDuringSchedulingIgnoredDuringExecution：硬亲和力配置，强制匹配
- preferredDuringSchedulingIgnoredDuringExecution：软亲和力配置，优先匹配
- podAffinity：pod亲和力配置，表示和具有匹配标签的pod部署在一起
- podAntiAffinity：pod反亲和力，表示和具有匹配标签的pod分开部署
- labelSelector：Pod选择器配置，可以配置多个
- matchExpressions：和节点亲和力配置一致
- operator：配置和节点亲和力一致，但是没有Gt和Lt
- topologyKey：匹配的拓扑域的key，也就是节点上label的key，key和value相同的为同一个域，可以用于标注不同的机房和地区
- namespaces: 和哪个命名空间的Pod进行匹配，为空为当前命名空间

```bash
# pod全配置示例，仅是示例，实际使用按需选择即可
apiVersion: v1
kind: Pod
metadata:
  name: pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          namespaces:
          - default
          topologyKey: failure-domain.beta.kubernetes.io/zone
  containers:
  - name: pod-affinity
    image: nginx
```

## 综合实例

### 实例一同一个应用副本互斥地部署在多个节点

这种操作避免同一个应用pod副本部署在同一个节点，如果节点故障，造成应用的不可用。即避免单点故障。

```bash
# 先查看污点，有4个节点可以使用，不影响后续操作，可以不删污点
[root@k8s-master01 schedule]# kubectl describe nodes | grep taint -i
Taints:             role/k8s-master01:NoSchedule
Taints:             role/k8s-master02:PreferNoSchedule
Taints:             <none>
Taints:             <none>
Taints:             <none>
# 创建3副本的应用myapp
[root@k8s-master01 schedule]# vim myapp.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myapp-podantiaffinity # deployment标签和pod标签可以不一致
  name: myapp-podantiaffinity
  namespace: kube-public
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp # deployment匹配标签和要和下面template里pod标签一致
  template:
    metadata:
      labels:
        app: myapp
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions: # 匹配pod标签
              - key: app
                operator: In
                values:
                - myapp
            topologyKey: kubernetes.io/hostname # 拓扑域，匹配主机名key
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: myapp
[root@k8s-master01 schedule]# kubectl apply -f myapp.yaml 
deployment.apps/myapp-podantiaffinity created
[root@k8s-master01 schedule]# kubectl get deployments.apps -n kube-public myapp-podantiaffinity 
NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
myapp-podantiaffinity   3/3     3            3           105s
[root@k8s-master01 schedule]# kubectl get rs,pod -n kube-public -owide --show-labels | grep myapp
replicaset.apps/myapp-podantiaffinity-6f8dc8f64f   3         3         3       2m35s   myapp        nginx    app=myapp,pod-template-hash=6f8dc8f64f       app=myapp,pod-template-hash=6f8dc8f64f
pod/myapp-podantiaffinity-6f8dc8f64f-772m6   1/1     Running   0             2m35s   172.27.14.211    k8s-node02     <none>           <none>            app=myapp,pod-template-hash=6f8dc8f64f
pod/myapp-podantiaffinity-6f8dc8f64f-7hxd4   1/1     Running   0             2m35s   172.17.125.50    k8s-node01     <none>           <none>            app=myapp,pod-template-hash=6f8dc8f64f
pod/myapp-podantiaffinity-6f8dc8f64f-rk7rf   1/1     Running   0             2m35s   172.18.195.21    k8s-master03   <none>           <none>            app=myapp,pod-template-hash=6f8dc8f64f

# 可用的调度节点为4，修改副本数为5，会发现有一个无法调度，一直处于pending状态
[root@k8s-master01 schedule]# kubectl scale deployment -n kube-public myapp-podantiaffinity --replicas=5
deployment.apps/myapp-podantiaffinity scaled
[root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp
myapp-podantiaffinity-6f8dc8f64f-772m6   1/1     Running   0              16m    172.27.14.211    k8s-node02     <none>           <none>
myapp-podantiaffinity-6f8dc8f64f-7hxd4   1/1     Running   0              16m    172.17.125.50    k8s-node01     <none>           <none>
myapp-podantiaffinity-6f8dc8f64f-rk7rf   1/1     Running   0              16m    172.18.195.21    k8s-master03   <none>           <none>
myapp-podantiaffinity-6f8dc8f64f-sqxjm   0/1     Pending   0              10m    <none>           <none>         <none>           <none>
myapp-podantiaffinity-6f8dc8f64f-zrtkr   1/1     Running   0              10m    172.25.92.77     k8s-master02   <none>           <none>
# 查看处于pending状态的pod事件，提示不匹配pod anti-affinity rules
[root@k8s-master01 schedule]# kubectl describe po -n kube-public myapp-podantiaffinity-6f8dc8f64f-sqxjm | grep -i events -A 10 
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  12s (x9 over 9m13s)  default-scheduler  0/5 nodes are available: 1 node(s) had taint {role/k8s-master01: }, that the pod didn't tolerate, 4 node(s) didn't match pod anti-affinity rules.
```

### 实例二同一个应用副本互斥地部署在指定标签的节点

```bash
# 之前配置过只有k8s-node01和k8s-node02有disktype=ssd标签
[root@k8s-master01 schedule]# vim myapp.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myapp-podantiaffinity # deployment标签和pod标签可以不一致
  name: myapp-podantiaffinity
  namespace: kube-public
spec:
  replicas: 2 # 副本数改为2
  selector:
    matchLabels:
      app: myapp # deployment匹配标签和要和下面template里pod标签一致
  template:
    metadata:
      labels:
        app: myapp
    spec:
      nodeSelector: # 增加nodeSelector标签选择器，节点标签需要已存在
        disktype: ssd
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions: # 匹配pod标签
              - key: app
                operator: In
                values:
                - myapp
            topologyKey: kubernetes.io/hostname # 拓扑域，匹配主机名key
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: myapp
[root@k8s-master01 schedule]# kubectl apply -f myapp.yaml 
deployment.apps/myapp-podantiaffinity created
[root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp
myapp-podantiaffinity-65b6c697b7-gtqzc   1/1     Running   0              48s    172.27.14.212    k8s-node02     <none>           <none>
myapp-podantiaffinity-65b6c697b7-mcv6j   1/1     Running   0              48s    172.17.125.51    k8s-node01     <none>           <none>
# 副本数改为3，由于没有多余disktype=ssd节点互斥调度，会处于pending状态
[root@k8s-master01 schedule]# kubectl scale deployment -n kube-public myapp-podantiaffinity --replicas=3
deployment.apps/myapp-podantiaffinity scaled
[root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp
myapp-podantiaffinity-65b6c697b7-gtqzc   1/1     Running   0              2m17s   172.27.14.212    k8s-node02     <none>           <none>
myapp-podantiaffinity-65b6c697b7-mcv6j   1/1     Running   0              2m17s   172.17.125.51    k8s-node01     <none>           <none>
myapp-podantiaffinity-65b6c697b7-t7v7h   0/1     Pending   0              4s      <none>           <none>         <none>           <none>
# 情况和实例一一致
[root@k8s-master01 schedule]# kubectl describe po -n kube-public myapp-podantiaffinity-65b6c697b7-t7v7h | grep -i event -A 10
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  18s (x2 over 83s)  default-scheduler  0/5 nodes are available: 1 node(s) had taint {role/k8s-master01: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity/selector, 2 node(s) didn't match pod anti-affinity rules.
```

### 实例三尽量将应用调度到ssd高性能节点

这种情况是部分应用侧重高性能服务器部署服务，但不在乎是否是部署到同一节点

```bash
# k8s-node01和k8s-node02已有disktype=ssd高性能标签，假设这两台服务器就是高性能的
# k8s-master03是传统一般性能的服务器，打上标签
[root@k8s-master01 schedule]# kubectl label nodes k8s-master03 disktype=physical
node/k8s-master03 labeled
[root@k8s-master01 schedule]# vim myapp.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myapp-podantiaffinity # deployment标签和pod标签可以不一致
  name: myapp-podantiaffinity
  namespace: kube-public
spec:
  replicas: 2 # 副本数改为2
  selector:
    matchLabels:
      app: myapp # deployment匹配标签和要和下面template里pod标签一致
  template:
    metadata:
      labels:
        app: myapp
    spec:
      nodeSelector: # 增加nodeSelector标签选择器，节点标签需要已存在
        disktype: ssd
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
            weight: 100 # 高性能服务器权重
          - preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - physical
            weight: 10 # 传统服务器权重
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: myapp
[root@k8s-master01 schedule]# kubectl apply -f myapp.yaml 
deployment.apps/myapp-podantiaffinity created
[root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp
myapp-podantiaffinity-7b655bd4fc-mw7g8   1/1     Running   0              34s    172.27.14.213    k8s-node02     <none>           <none>
myapp-podantiaffinity-7b655bd4fc-sqg5f   1/1     Running   0              34s    172.17.125.52    k8s-node01     <none>           <none>
```

### 实例四同一个应用调度在不同拓扑域

这种情况是应用更侧重的是不同地域服务的访问速度，就近部署是最好的，同时也避免了单点故障。前提是为所有机器划分拓扑域，否则未划分的机器也会被正常调度。

```bash
# 假设k8s-node01和k8s-node02分别属于上海和北京数据中心，打上标签
[root@k8s-master01 schedule]# kubectl label nodes k8s-node01 k8s-master03 region=ShangHai
node/k8s-node01 labeled
node/k8s-master03 labeled
[root@k8s-master01 schedule]# kubectl label nodes k8s-node02 region=BeiJing
node/k8s-node02 labeled
[root@k8s-master01 schedule]# vim myapp.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myapp-pod-region-antiaffinity # deployment标签和pod标签可以不一致
  name: myapp-pod-region-antiaffinity
  namespace: kube-public
spec:
  replicas: 2
  selector:
    matchLabels:
      region: different # deployment匹配标签和要和下面template里pod标签一致
  template:
    metadata:
      labels:
        region: different
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions: # 匹配pod标签
              - key: region
                operator: In
                values:
                - different
            topologyKey: region # 拓扑域，匹配region
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: myapp
[root@k8s-master01 schedule]# kubectl apply -f myapp.yaml 
deployment.apps/myapp-pod-region-antiaffinity created
[root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp
myapp-pod-region-antiaffinity-6675849bf9-577j2   1/1     Running   0              16s    172.18.195.27    k8s-master03   <none>           <none>
myapp-pod-region-antiaffinity-6675849bf9-5rmww   1/1     Running   0              16s    172.27.14.218    k8s-node02     <none>           <none>
# 修改副本数为3
[root@k8s-master01 schedule]# kubectl scale deployment -n kube-public myapp-pod-region-antiaffinity --replicas=3
deployment.apps/myapp-pod-region-antiaffinity scaled
[root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp
myapp-pod-region-antiaffinity-6675849bf9-577j2   1/1     Running   0              91s    172.18.195.27    k8s-master03   <none>           <none>
myapp-pod-region-antiaffinity-6675849bf9-5rmww   1/1     Running   0              91s    172.27.14.218    k8s-node02     <none>           <none>
myapp-pod-region-antiaffinity-6675849bf9-7s9pc   1/1     Running   0              2s     172.25.92.78     k8s-master02   <none>           <none>
[root@k8s-master01 schedule]# kubectl get nodes k8s-master02 --show-labels 
NAME           STATUS   ROLES    AGE   VERSION   LABELS
k8s-master02   Ready    master   9d    v1.23.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master02,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node.kubernetes.io/node=
# k8s-master02并没有设置region，也被调度了，可见如果存在没有标记拓扑域的服务器，当拓扑域副本数溢出时，也会被正常调度
# 先调回正常副本数
[root@k8s-master01 schedule]# kubectl scale deployment -n kube-public myapp-pod-region-antiaffinity --replicas=2
deployment.apps/myapp-pod-region-antiaffinity scaled
[root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp
myapp-pod-region-antiaffinity-6675849bf9-577j2   1/1     Running   0              5m45s   172.18.195.27    k8s-master03   <none>           <none>
myapp-pod-region-antiaffinity-6675849bf9-5rmww   1/1     Running   0              5m45s   172.27.14.218    k8s-node02     <none>           <none>
# 为k8s-master02设置不可调度污点，扩充副本测试之
[root@k8s-master01 schedule]# kubectl taint node k8s-master02 region:NoSchedule
node/k8s-master02 tainted
# 此时只存在三个节点可以调度（master01是之前就设置的污点），而且都在region拓扑域中
[root@k8s-master01 schedule]# kubectl describe nodes | grep -i taint -A 1
Taints:             role/k8s-master01:NoSchedule
Unschedulable:      false
--
Taints:             region:NoSchedule # 不可调度污点会覆盖掉PreferNoSchedule污点，不删除也可以
                    role/k8s-master02:PreferNoSchedule
--
Taints:             <none>
Unschedulable:      false
--
Taints:             <none>
Unschedulable:      false
--
Taints:             <none>
Unschedulable:      false
# 扩充副本测试
[root@k8s-master01 schedule]# kubectl scale deployment -n kube-public myapp-pod-region-antiaffinity --replicas=3
deployment.apps/myapp-pod-region-antiaffinity scaled
[root@k8s-master01 schedule]# kubectl get po -n kube-public -owide | grep myapp
myapp-pod-region-antiaffinity-6675849bf9-577j2   1/1     Running   0              9m41s   172.18.195.27    k8s-master03   <none>           <none>
myapp-pod-region-antiaffinity-6675849bf9-5rmww   1/1     Running   0              9m41s   172.27.14.218    k8s-node02     <none>           <none>
myapp-pod-region-antiaffinity-6675849bf9-djkgc   0/1     Pending   0              5s      <none>           <none>         <none>           <none>
[root@k8s-master01 schedule]# kubectl describe po -n kube-public myapp-pod-region-antiaffinity-6675849bf9-djkgc | grep -i event -A 5
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  43s   default-scheduler  0/5 nodes are available: 1 node(s) had taint {region: }, that the pod didn't tolerate, 1 node(s) had taint {role/k8s-master01: }, that the pod didn't tolerate, 3 node(s) didn't match pod anti-affinity rules.
```

> 需要注意的是标签的唯一性，不要和其他项目应用冲突。
