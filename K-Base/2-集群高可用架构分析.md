# Kubernetes基础之集群高可用架构

## 定义

Kubernetes 是开源生产级容器编排引擎，可以实现容器的自动发布、伸缩和管理。
Kubernetes 是谷歌开源的容器集群管理系统，是 Google 多年大规模容器管理技术 Borg 的开源版本。Borg作业调度系统详见论文：[Google使用Borg进行大规模集群管理](https://deemoprobe.oss-cn-shanghai.aliyuncs.com/doc/Large-scale%20cluster%20management%20at%20Google%20with%20Borg.pdf?versionId=CAEQNBiBgMCU4u_3.xciIGM1MGQ0YmI0NDQ4MjQxNzQ4MzhhNmI2MWVmYThiZmNl)

Kubernetes理念：

- 集群管理：以计算节点为基础，管理集群彼此通信的节点
- 作业调度和管理
  - 支持多种存储类型：如本地存储和网络存储
  - 滚动更新和回退
  - 高利用率调度机制
  - 资源配额和QoS
  - 自愈机制：健康检查和故障恢复
  - 密码和配置管理
- 服务发现与治理
- 声明式API
- 控制器模式
- 插件化架构
- 标准化

Kubernetes高可用架构图如图所示

[![1][1]][1]

[1]: https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/Kubernetes高可用架构.png

## 模型设计

- TypeMeta定义对象类型
  - Group：对象分组，如apps、node.k8s.io
  - Kind：对象类型，如Node、Pod、Deployment
  - Version：对象版本，如v1、v1beta1
- Metadata定义对象身份
  - Namespace：对象所在命名空间
  - Name：对象名称
  - Label：对象标签
  - Annotation：对象注释信息
  - Finalizer：资源锁，当对象接收删除请求时，如果该字段不为空，则会等待列表中资源释放
  - ResourceVersion：资源版本，保证对象多线程操作时的一致性，是一种乐观锁
  - 时间戳、UID
- Spec定义对象的状态

## ETCD

ETCD是高可用的键值对分布式存储系统，用于持久化存储集群中的资源对象。诸如集群中Node、Pod、Service等对象的状态和元数据，以及配置数据等。ETCD高可用实现的两种方式：

- Kubernetes集群内建高可用etcd集群：在每个Master节点中部署etcd实例，Master节点高可用的同时部署etcd的高可用。这种方式将etcd与Kubernetes主控节点耦合在一起。
- Kubernetes集群外部高可用etcd集群：即etcd集群是独立于Kubernetes集群存在的，这种方式将etcd与Kubernetes集群解耦，使得二者故障影响系数降低，更专注于各自本身的集群管理工作。缺点是架构中需要的独立主机数量增加。

[![2][2]][2]

[![3][3]][3]

[2]: https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/内建式etcd高可用集群架构.drawio.png

[3]: https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/外部etcd高可用集群架构.drawio.png

任何etcd实例都可以处理读请求，只有领导者可以处理写请求；当etcd实例接收到Kubernetes集群apiserver的写请求时候，如果该实例不是领导者，则请求会转交到领导者处理；领导者将请求复制到其他etcd成员节点进行仲裁，当仲裁过半数实例同意后，领导者才会对请求进行操作。每个集群仅有一名领导者，当领导者不再响应时，其余etcd节点会在选举倒计时结束后开始新领导者的选举，将自己标记为候选者，集群内投票选举。

ETCD集群实例之间通过Raft一致性共识算法确保数据的一致性，一般是3或5个etcd实例组成高可用集群：奇数个是为了保证`etcd leader`选举的合理性；不使用更多的etcd实例原因是一方面为了节省计算资源，另一方面是Raft算法机制导致如果实例太多，集群写仲裁时间边长，性能会一定程度上变低。此外，etcd集群所能容忍的故障节点数最多为`(N-1)/2`，N为etcd集群实例数。

一个三节点etcd高可用集群的状态：

[![4][4]][4]

[4]: https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20220313212422.png

## Master节点

集群的控制节点，核心组件如下：

- apiserver：集群控制平面的前端，承担API网关的职责。是用户请求和系统组件与集群交互的唯一入口，所有资源的创建、删除和更新等操作都是通过调用apiserver的API接口进行。集群内，apiserver是各个模块之间的通信枢纽，提供etcd API接口，这些API能够让集群内其他组件监听到资源对象的增删改的变化；集群外，apiserver充当网关的作用，拥有完整的安全机制，完成客户端身份的认证（Authentication）和授权（Authorization），并对资源进行准入控制（Addmission Control）。
- controller-manager：集群自动化管理和控制中心，包含多种控制器。诸如Pod控制器（rs/deploy/ds/sts/cj/hpa）、网络管理方面（ep/svc/ingress等）和存储方面（pv/pvc等），还有其他几十种控制器。控制器采用主备模式和领导选举机制实现故障转移，允许多个副本同时运行，但只有领导者在工作，其他副本在领导者无法工作时选举新领导者提供服务。
- scheduler 集群Pod调度器，监听apiserver处的Pod变化，根据预定的条件对Pod进行调度。调度程序会综合考虑Pod的资源需求（Quota：CPU或内存）、服务质量（QoS）、亲和力反亲和力、策略约束和集群的运行状况等，将Pod调度到合适的计算节点，实现资源的高利用率。调度器也采用主备模式和领导选举机制。

master节点资源一定要尽量给够，以至于后期不会拖累集群的整体性能。并且允许的情况下，etcd最好也要和master节点区分开来，单独创建集群进行数据的存储，etcd-cluster必须使用高性能ssd硬盘，否则后期将大大影响集群的性能。

```bash
# 集群一个master节点中核心组件守护进程状态和参数
[root@k8s-master01 ~]# systemctl status -l kube-apiserver.service
● kube-apiserver.service - Kubernetes API Server
   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
   Active: active (running) since Sun 2022-03-13 19:59:48 CST; 33min ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 1782 (kube-apiserver)
    Tasks: 15
   Memory: 375.2M
   CGroup: /system.slice/kube-apiserver.service
           └─1782 /usr/local/bin/kube-apiserver --v=2 --logtostderr=true --allow-privileged=true --bind-address=0.0.0.0 --secure-port=6443 --insecure-port=0 --advertise-address=192.168.43.183 --service-cluster-ip-range=10.96.0.0/12 --service-node-port-range=30000-32767 --etcd-servers=https://192.168.43.183:2379,https://192.168.43.184:2379,https://192.168.43.185:2379 --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem --etcd-certfile=/etc/etcd/ssl/etcd.pem --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem --client-ca-file=/etc/kubernetes/pki/ca.pem --tls-cert-file=/etc/kubernetes/pki/apiserver.pem --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-account-issuer=https://kubernetes.default.svc.cluster.local --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota --authorization-mode=Node,RBAC --enable-bootstrap-token-auth=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem --requestheader-allowed-names=aggregator --requestheader-group-headers=X-Remote-Group --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-username-headers=X-Remote-User --feature-gates=EphemeralContainers=true
   ...
[root@k8s-master01 ~]# systemctl status -l kube-controller-manager.service
● kube-controller-manager.service - Kubernetes Controller Manager
   Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled)
   Active: active (running) since Sun 2022-03-13 19:58:12 CST; 34min ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 1002 (kube-controller)
    Tasks: 6
   Memory: 66.7M
   CGroup: /system.slice/kube-controller-manager.service
           └─1002 /usr/local/bin/kube-controller-manager --v=2 --logtostderr=true --address=127.0.0.1 --root-ca-file=/etc/kubernetes/pki/ca.pem --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem --service-account-private-key-file=/etc/kubernetes/pki/sa.key --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig --leader-elect=true --use-service-account-credentials=true --node-monitor-grace-period=40s --node-monitor-period=5s --pod-eviction-timeout=2m0s --controllers=*,bootstrapsigner,tokencleaner --allocate-node-cidrs=true --cluster-cidr=172.16.0.0/12 --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem --node-cidr-mask-size=24 --feature-gates=EphemeralContainers=true
   ...
[root@k8s-master01 ~]# systemctl status -l kube-scheduler.service 
● kube-scheduler.service - Kubernetes Scheduler
   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)
   Active: active (running) since Sun 2022-03-13 19:58:12 CST; 35min ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 998 (kube-scheduler)
    Tasks: 8
   Memory: 50.6M
   CGroup: /system.slice/kube-scheduler.service
           └─998 /usr/local/bin/kube-scheduler --v=2 --logtostderr=true --address=127.0.0.1 --leader-elect=true --kubeconfig=/etc/kubernetes/scheduler.kubeconfig --feature-gates=EphemeralContainers=true
   ...
```

## Node节点

应用部署的节点，工作节点，资源调度的对象，核心组件如下：

- kubelet：运行在节点上负责启动容器的守护进程，监听节点上pod的状态，与master节点apiserver进行通信将状态上报到主控节点。kubelet需定时（nodeStatusUpdateFrequency: 10s 默认10s）向apiserver汇报自身的情况（磁盘空间状态、CPU和Memory是否有压力和自身服务是否Ready等），如果Node上的kubelet停止了汇报，NodeLifecycle控制器将标记对应node状态为NotReady，过一段时间后驱逐该node节点上的pod。pod被调度到kubelet所在节点时，kubelet会首先将pod中申请的volume挂载到当前节点，之后调用容器运行时创建pause沙箱（PodSandBox）容器（该容器用以维护pod网络协议栈）和pod容器。kubelet周期性地查询容器的状态，并定期汇报容器状态，通过cAdvisor监控容器资源的使用情况。
- kube-proxy：负责pod之间的通信和负载均衡，将指定的流量分发到后端正确的机器上，一般工作模式默认为ipvs。具体工作行为是从apiserver监听service和endpoint对象，根据endpoint信息设置service到后端pod的路由，维护网络规则。

上面两个守护进程并不是工作节点特有，一般master控制节点也会启动kubelet和kube-proxy进程提高集群的可用性。

IPVS：监听master节点增加和删除service以及endpoint的消息，调用netlink接口创建相应的ipvs规则。通过ipvs规则将流量转到相应的pod上。ipvs是内核级的转发，速度很快。
Iptables：监听master节点增加和删除service以及endpoint的消息，对于每一个service，都会创建一个iptables规则，将service的clusterIP代理到后端对应的pod上。iptables由于线性查找匹配、全量更新等特点，当规则很多时，性能会比ipvs差，所以一般选择ipvs即可。

```bash
# 集群一个node节点核心组件运行状态和参数
[root@k8s-node01 ~]# systemctl status -l kubelet.service
● kubelet.service - Kubernetes Kubelet
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-kubelet.conf
   Active: active (running) since Sun 2022-03-13 20:03:34 CST; 33min ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 1572 (kubelet)
    Tasks: 14
   Memory: 164.2M
   CGroup: /system.slice/kubelet.service
           └─1572 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --config=/etc/kubernetes/kubelet-conf.yml --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock --cgroup-driver=systemd --node-labels=node.kubernetes.io/node=
   ...
[root@k8s-node01 ~]# systemctl status -l kube-proxy.service
● kube-proxy.service - Kubernetes Kube Proxy
   Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled)
   Active: active (running) since Sun 2022-03-13 20:03:32 CST; 33min ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 996 (kube-proxy)
    Tasks: 6
   Memory: 57.9M
   CGroup: /system.slice/kube-proxy.service
           └─996 /usr/local/bin/kube-proxy --config=/etc/kubernetes/kube-proxy.yaml --v=2 --feature-gates=EphemeralContainers=true
   ...
# kubelet进程对应的配置文件--config=/etc/kubernetes/kubelet-conf.yml
[root@k8s-node01 ~]# cat /etc/kubernetes/kubelet-conf.yml 
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.pem
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: true
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 2m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
volumeStatsAggPeriod: 1m0s
featureGates:
  EphemeralContainers: true
# kube-proxy对应的配置文件--config=/etc/kubernetes/kube-proxy.yaml
[root@k8s-node01 ~]# cat /etc/kubernetes/kube-proxy.yaml 
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
clientConnection:
  acceptContentTypes: ""
  burst: 10
  contentType: application/vnd.kubernetes.protobuf
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
  qps: 5
clusterCIDR: 172.16.0.0/12 
configSyncPeriod: 15m0s
conntrack:
  max: null
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
enableProfiling: false
healthzBindAddress: 0.0.0.0:10256
hostnameOverride: ""
iptables:
  masqueradeAll: false
  masqueradeBit: 14
  minSyncPeriod: 0s
  syncPeriod: 30s
ipvs:
  masqueradeAll: true
  minSyncPeriod: 5s
  scheduler: "rr"
  syncPeriod: 30s
kind: KubeProxyConfiguration
metricsBindAddress: 127.0.0.1:10249
mode: "ipvs"
nodePortAddresses: null
oomScoreAdj: -999
portRange: ""
udpIdleTimeout: 250ms

```

## 容器运行时

符合CRI（Container Runtime Interface）标准的容器运行时（Container Runtime）是实际上管理容器的组件，容器运行时可分为高层和底层运行时。高层运行时诸如：Docker、containerd、CRI-O，官方介绍:[容器运行时](https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/)；底层运行时诸如：runc、kata、gVisor，kata和gVisor相对不是很成熟，目前底层运行时一般默认选择runc。

```bash
# containerd容器运行时服务状态
[root@k8s-master01 ~]# systemctl status -l containerd.service 
● containerd.service - containerd container runtime
   Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; vendor preset: disabled)
   Active: active (running) since Sun 2022-03-13 19:58:14 CST; 1h 13min ago
     Docs: https://containerd.io
  Process: 996 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
 Main PID: 1011 (containerd)
    Tasks: 48
   Memory: 99.9M
   CGroup: /system.slice/containerd.service
           ├─1011 /usr/bin/containerd
           ├─1857 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 83f9705395fb4a165bd52894f6545a58b8040a5fe6774b03cdcd02ca90900708 -address /run/containerd/containerd.sock
           ├─2394 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id ae7a1e3a28a331bf11762c3b049e8237c322cddabd842dca398911a45dc656db -address /run/containerd/containerd.sock
           └─2802 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 30e5560350fdcb76efbbb10f7c8e9a41fa4fdc10e4483f62830fff749a448fb1 -address /run/containerd/containerd.sock
```

## 网络插件

符合CNI（Container Network Interface）标准的网络插件，诸如：Calico、Cilium、Weave和Flannel等。会为每个pod生成唯一的IP地址，并且把每个节点当做一个路由器。可以使用`route -n`查看

```bash
# 使用calico插件的路由
[root@k8s-master01 ~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.43.1    0.0.0.0         UG    0      0        0 ens33
169.254.0.0     0.0.0.0         255.255.0.0     U     1002   0        0 ens33
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
172.17.125.0    192.168.43.186  255.255.255.192 UG    0      0        0 tunl0
172.18.195.0    192.168.43.185  255.255.255.192 UG    0      0        0 tunl0
172.25.92.64    192.168.43.184  255.255.255.192 UG    0      0        0 tunl0
172.25.244.192  0.0.0.0         255.255.255.192 U     0      0        0 *
172.25.244.213  0.0.0.0         255.255.255.255 UH    0      0        0 cali183ffe150c7
172.25.244.214  0.0.0.0         255.255.255.255 UH    0      0        0 calicec0b1cfb68
172.27.14.192   192.168.43.187  255.255.255.192 UG    0      0        0 tunl0
192.168.43.0    0.0.0.0         255.255.255.0   U     0      0        0 ens33
```

## CoreDNS

用于集群内部service的解析，可以让pod把service名称解析成ip地址，然后通过service的IP地址链接到对应的应用上。

## Pod

Pod是Kubernetes中最小的单元，是由一个或多个容器组成的。每个pod还包含一个pause容器，pause容器是pod的父容器，负责僵尸进程的回收管理，通过pause容器可以使同一个pod内多个容器共享存储、网络、PID、IPC等。详细可以查看博客：[Pod定义及零宕机部署](https://www.deemoprobe.com/share/k-pod/)

> 本文相关概念参考书籍：[《Kubernetes生产化实践之路》](http://www.broadview.com.cn/book/6213)
